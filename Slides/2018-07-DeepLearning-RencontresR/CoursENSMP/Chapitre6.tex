\chapter{Classification automatique}Une confusion r\'epandue existe entre les termes classementet classification (respectivement ``classification'' et ``clustering''  en anglais).Le classement pr\'esuppose l'existence de classes dont certains objetssont connus, alors que laclassification tente de d\'ecouvrir une structure de classes qui soit ``naturelle'' aux donn\'ees. Dans la litt\'erature li\'ee \`ala reconnaissance des formes, la distinction entre les deux approchesest souvent d\'esign\'ee par les termes``apprentissage supervis\'e'' et ``non supervis\'e''. Une classification peut avoir diff\'erentes motivations   : compresser des informations,d\'ecrire de mani\`ere simplifi\'ee de grandes masses de donn\'ees,structurer un ensemble de connaissances,r\'ev\'eler des structures, des causes cach\'ees,r\'ealiser un diagnostic... % baratin sur les fonctions discriminantesDans le contexte du classement, les fonctions discriminantes d\'efinissentla similitude entre un vecteur forme et les diff\'erentes classes (chapitre 4).Ces fonctions discriminantes peuvent \^etre construite \`a partir d'unemesure de ressemblance entre les vecteurs forme. En classification le m\^emetype d'approche  est \`a la base des m\'ethodes, et l'\'etape pr\'ealable\`a toute classification consiste \`a d\'efinir une mesurede  ressemblance entre les  objets (vecteurs formes). Traditionnellementdeux d\'emarches sont envisag\'ees :\begin{itemize}\item on peut dire que deux objets sont semblables s'ils  partagent une certainecaract\'eristique. Consid\'erons le nombre de doigts d'un \^etre vivant et comparonsle singe et l'homme : sur ce crit\`ere de comparaison (et sur bien d'autres) les deuxesp\`eces seront jug\'ees semblables. Ce genre de d\'emarche aboutit \`a une classification{\em monoth\'etique} base de l'approche aristot\'elicienne \cite{Sutcliffe1994}. Tousles objets d'une m\^eme classe partagent alors un certain nombre de caract\'eristiques(e.g. : ``Tous les hommes sont mortels'') ;   \item on peut aussi mesurer la ressemblance en utilisant une mesure de proximit\'e (distance,dissimilarit\'e). Dans ce cas la notion de ressemblance est mesur\'ee de fa\c{c}on plusfloue et deux objets d'une m\^eme classe poss\'ederont des caract\'eristiques ``proches''au sens de la mesure utilis\'ee. Cette d\'emarche est dite {\em polyth\'etique}.\end{itemize}Le contexte de  ce chapitre est  l'approche polyth\'etique etplus particuli\`erement,  les m\'ethodes de classificationqui mesurent la ressemblance \`a l'aide d'une distance..Une classification am\`ene \`a r\'epartir l'ensemble des vecteurs forme en diff\'erentesclasses {\em homog\`enes}. La d\'efinition d'une classe et les relations entre classes peuvent \^etre tr\`es vari\'ees. Dans ce chapitre nous nous int\'eresserons aux deux principales structures de classification :\begin{itemize}\itemla partition, \itemla hi\'erarchie. \end{itemize}\section{Partitions}\begin{defi}${\cal F}$ \'etant un ensemble fini, un ensemble $P=({\cal C}_1,{\cal C}_2,...,{\cal C}_K)$ de parties non vides de ${\cal F}$ est une partition si :\begin{enumerate}\item $\forall i\neq j$, ${\cal C}_i \cap {\cal C}_j=\emptyset,$\item $\cup_i {\cal C}_i={\cal F}.$ \end{enumerate}\end{defi}Dans un ensemble  ${\cal F}=(\x_1,...,\x_N)$  partitionn\'e en $K$ classes,chaque \'el\'ement de l'ensembleappartient \`a une classe et une seule. Une mani\`ere pratique de d\'ecrirecette partition $P$ consiste \`a utiliser une notation matricielle. Soit $\bc(P)$la matrice caract\'eristique de la partition $P=({\cal C}_1,{\cal C}_2,...,{\cal C}_K)$ (ou matrice de classification) :\[\bc(P)=\bc=\left(\begin{array}{l l l} c_{11} & \cdots & c_{1K} \\\vdots & \ddots & \vdots \\c_{N1} & \cdots & c_{NK} \\\end{array}\right)\] o\`u $c_{ik}=1$ si et seulement si $\x_i \in {\cal C}_k$, et $c_{ik}=0$ sinon. Remarquons que la somme de la $i$\ieme ligne est \'egale \`a 1 (un \'el\'ement appartient\`a une seule classe) et la somme des valeurs de la $k$\ieme colonne vaut$n_k$ le nombre d'\'el\'ements de la classe ${\cal C}_k$. On a donc $\sum_{k=1}^K n_k = N$.La notion de partition dure repose sur une conception ensemblisteclassique. Consid\'erant les travaux de \citeasnoun{Zadeh1965} sur les ensemblesflous, une d\'efinition du concept de partition floue semble``naturelle''. La classification floue, d\'evelopp\'ee au d\'ebut desann\'ees 1970 \cite{Ruspini1969}, g\'en\'eralise une approcheclassique en classification en \'elargissant la notiond'appartenance \`a une classe.Dans le cadre de la conception ensembliste classique, un individu $\x_i$appartient appartient ou n'appartient pas \`a un ensemble donn\'e ${\cal C_k}$.Dans la th\'eorie des sous-ensembles flous, un individu peutappartenir \`a plusieurs classes avec diff\'erent degr\'es d'appartenance.En classification cela revient autoriser les vecteurs formes\`a appartenir \`a toutes les classes, ce qui se traduit par lerel\^achement de la contrainte de binarit\'e sur les coefficients d'appartenance $c_{ik}$. Une partition floue est d\'efinie parune matrice de classification floue $\bc=\{c_{ik}\}$ v\'erifiant les conditions suivantes :\begin{enumerate}\item $\forall k=1..K$, $\forall \x_i \in {\cal F}$, $c_{ik}\in [0,1]$.\item $\forall k=1..K$, $0< \sum_{i=1}^N c_{ik} <N,$\item $\forall \x_i \in {\cal F}$, $\sum_{k=1}^K c_{ik}.$ \end{enumerate}La seconde condition traduit le fait qu'aucune classene doit \^etre vide et la troisi\`eme exprime le concept d'appartenance totale.\subsection{Crit\`eres et algorithmes}%-------------------------------------------Les concepts de partition et de classification polyth\'etique\'etant pr\'ecis\'es, la question suivante \'emerge :comment trouver une partition optimale d'un ensemble de donn\'ees,lorsque la ressemblance entre deux individus est \'evalu\'ee parune mesure de proximit\'e  ?La premi\`ere chose \`a faire consiste \`a clarifier formellementle sens du mot optimal. La solution g\'en\'eralement adopt\'ee est de choisir une mesure num\'erique de la qualit\'e d'une partition.Cette mesure est parfois appel\'ee crit\`ere, fonctionnelle, oubien encore fonction d'\'energie. L'objectif d'une proc\'edure de classificationest donc de trouver la partition ou les partitions qui donnent la meilleure valeur (la plus petite ou la plus grande) pour un crit\`ere donn\'e. Mais le nombre de partitions possibles, m\^eme pour un probl\`emede taille raisonnable, est \'enorme. En effet si l'on consid\`ere unensemble de $N$ objets \`a partitionner en $K$ classes, le nombre departitions possibles est :\begin{equation}NP(N,K)=\frac{1}{K  !}\sum_{k=0}^{K}(-1)^{k-1}  \cdot C_k^K \cdot k^N.\end{equation}\begin{ex}Soit un ensemble de 8 objets que l'on d\'esire partager en 4 classes. Il existe1701 partitions possibles   !\end{ex}Plut\^ot que de chercher la meilleure partition, celle qui donne la valeur optimale du crit\`ere, on utilise des m\'ethodes plusrapides qui convergent vers des optima ``locaux'' du crit\`ere. Les partitions ainsi trouv\'ees sont souvent satisfaisantes.  \subsection{Inertie intra-classe et partition}De nombreux crit\`eres existent \cite{Gordon1980}. Certains peuvent \^etre li\'es, commenous le verrons dans la suite, au choix d'un mod\`ele pour l'ensemble des donn\'ees.  L'une des fonctions les plus utilis\'ee est la somme des variances intra-classes :\begin{eqnarray*}I_W & = & \sum_{k=1}^K \sum_{i=1}^N c_{ik} \| \x_i - \bm_k  \|^2 \\    & = & trace(\bS_W)\end{eqnarray*}o\`u les $\bm_k$ sont les prototypes (centres) des classes etles $c_{ik}$ sont les \'el\'ements d'une matrice de partition dure.Le probl\`eme pos\'e est alors un probl\`eme d'optimisation souscontraintes (li\'ees aux $c_{ik}$) :\begin{equation}(\hat{\bc}, \hat{\bm})=arg \min_{(\bc, \bm)}{ I_W(\bc,\bm)}\end{equation}o\`u $\bm$ repr\'esente l'ensemble des centres de gravit\'es.\subsubsection{L'algorithme des centres mobiles}\label{sec:algo}Un algorithme tr\`es r\'epandu pour r\'esoudre ce probl\`eme est celuides {\em k-means} ou centres mobiles. Historiquement, cet algorithme datedes ann\'ees soixante. Il a \'et\'e propos\'e par plusieurs chercheursdans diff\'erents domaines \`a des dates proches \cite{Edwards1965,Lloyd1957}. Cet algorithme bas\'e sur des consid\'erationsg\'eom\'etriques doit certainement son succ\`es \`a sa simplicit\'e et son efficacit\'e  :\begin{enumerate}\item Initialisation des centres : une m\'ethode r\'epandue consiste \`a initialiser lescentres avec les coordonn\'ees de $K$ points choisis au hasard.\item Ensuite les it\'erations poss\`edent la forme altern\'ee suivante :\begin{enumerate}\item \'etant donn\'e $\bm_1,\cdots,\bm_K$, choisir les $c_{ik}$ qui minimisent $I_W$,\item \'etant donn\'e $\bc=\{c_{ik}\}$, minimiser $I_W$ par rapport aux prototypes $\bm_1,\cdots,\bm_K$.\end{enumerate}\end{enumerate}La premi\`ere \'etape affecte chaque $\x_i$ au prototype le plus proche, etla seconde \'etape recalcule la position des prototypes en consid\'erantque le prototype de la classe $i$ devient son vecteur moyenne. Il est possible demontrer que chaque it\'eration fait d\'ecro\^{\i}tre le crit\`ere mais aucune garantie de convergence vers un maximum global n'existe en g\'en\'eral.Si le crit\`ere des k-means est consid\'er\'e du point de vue de la recherched'une partition floue, c'est-\`a-dire si les contraintes sur les $c_{ik}$ sontrel\^ach\'ees et deviennent $c_{ik}\in [0,1]$ \`a la place de $c_{ik}\in \{0,1\}$,la partition optimale au sens du nouveau crit\`ere est celle qui est optimale pour le crit\`ere classique \cite{Selim1984}. En d'autre terme, il n'y a aucun int\'er\^et \`a consid\'erer des partitions floues, lorsqu'on travailleavec le crit\`ere des k-means.Cette forme d'algorithme altern\'e o\`u un certain crit\`ere est optimis\'e,alternativement par rapport aux variables d'appartenance aux classes, puispar rapport aux param\`etres d\'efinissant ces classes a \'et\'e intensivementexploit\'e. Citons entre autre {\em les nu\'ees dynamiques} de Diday \cite{Diday1971}et l'algorithme des {\em fuzzy c-means} \cite{Bezdeck1974}.Notons que Webster \citeasnoun{Fisher1958} (\`a ne pas confondre avec Ronald Fisher)avait propos\'e un algorithme trouvant la partition optimale,au sens de la variance intra-classe, d'un ensemble de $N$ donn\'eesunidimensionnelles en $O(N \cdot K^2)$ op\'erations en utilisant des m\'ethodesissues de la programmation dynamique.  \subsubsection{Une version adaptative des centres mobiles}Une autre version des {\em k-means} \cite{Macqueen1967} consiste \`a modifierles prototypes des classes en consid\'erant les donn\'ees une \`a une.On parle alors d'algorithme adaptatif : \begin{enumerate}\item Les $K$ prototypes sont tir\'es au hasard parmi les $N$ points.\item A l'it\'eration $q$, un individu $\x_i$ est choisi au hasard.\begin{itemize}\item D\'etermination du prototype le plus proche de $\x_i$ :\[\bm_k^q=\min_j{\|\x_i - \bm_j^q \|}.\]L'individu est affect\'e \`a la classe $k$.\item Modification du prototype $\bm_k^q$ :\[\bm_k^{q+1}=\frac{\x_i + n_k^q \cdot \bm_j^q}{n_k^q +1},\]et\[n_k^{q+1}=n_k^q +1\]o\`u $n_k^q$ repr\'esente l'effectif de la classe $k$ \`a l'it\'eration $q$.\end{itemize}\end{enumerate}Les algorithmes adaptatifs sont particuli\`erement ad\'equats lorsquetoutes les donn\'ees \`a classer ne sont pas disponibles \`a l'avance.Les param\`etres d\'efinissant les classes peuvent alors \^etreajust\'es \`a l'apparition de chaque nouvelle donn\'ee sans trop de calculs. \subsubsection{Les nu\'ees dynamiques}L'algorithme des nu\'ees dynamiques \cite{Diday1971} est une g\'en\'eralisation de centres mobiles. L'id\'ee de base consiste \`a remplacer les prototypes$\bm_k$ (vecteurs de $\R^d$) par des \'el\'ements de  nature tr\`es diverse, nomm\'es noyaux. Le noyau $\bn_k$ d'une classe peut \^etre par exemple, un ensemblede vecteurs forme de l'ensemble d'apprentissage, une droite, une loi deprobabilit\'e... La partition est obtenue en minimisant un crit\`erede la forme\begin{equation}W(\bn,\bc)=\sum_{k=1}^K \sum_{i=1}^N c_{ik} D(\x_i ,\bn_k )\end{equation}par une proc\'edure d'optimisation altern\'ee. Chaque it\'eration minimisele crit\`ere en calculant successivement les noyaux en fixant la  partitiondonn\'ee, puis la partition en fixant les noyaux.     % ISODATA\subsection{Mod\`eles de m\'elange et partitions}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%De nombreux autres algorithmes et heuristiques qui optimisent d'autrescrit\`eres que celui de l'inertie intra-classes,  ou qui simplement produisent une partition, existent. L'approche probabiliste permetde s'adapter \`a une grande vari\'et\'e de situations etg\'en\'eralise certaines techniques usuelles (nu\'ees dynamiques, cartes de Kohonen). \\ L'approche probabiliste de la recherche de partitions faitl'hypoth\`ese que l'ensemble des donn\'ees $\x=(\x_1,...,\x_N)$ est lar\'ealisation d'un \'echantillon de $N$ variables al\'eatoires ind\'ependantesde m\^eme loi $f$, prenant leurs valeurs dans $\R^d$. La connaissance de cette loi $f$ doit permettre de s\'eparer ``naturellement'' les $N$observations en $k$ classes.  Les m\'elanges finis de densit\'e sont les distributions de probabilit\'eles plus  utilis\'ees dans ce contexte :\begin{equation}f(\x)=\sum_{k=1}^K p_k f_k(\x | \theta_k),\end{equation}avec $\sum_{k=1}^K p_k=1$. Ce genre de densit\'e appara\^{\i}t naturellementlorsque la population consid\'er\'ee est form\'ee de plusieurssous-populations qui ont des densit\'es diff\'erentes. Ceci explique l'int\'er\^et de ce mod\`ele en classification. Les mod\`eles de m\'elange,  de loi de Gauss ou de Bernoulli, sontles mod\`eles le plus souvent utilis\'es  dans le contexte de laclassification automatique.Dans ce contexte, deux approches ont \'et\'e propos\'ees :\begin{itemize}\iteml'approche m\'elange,\iteml'approche classification.\end{itemize} La premi\`ere approche consid\`ere effectivement que les individusobserv\'es sont les r\'ealisation d'un m\'elange de densit\'e, alors que la seconde approche traite chacune des sous-populationde mani\`ere s\'epar\'ee. \subsubsection{Approche  m\'elange}Si l'on consid\`ere que les vecteur forme observ\'es $(\x_1,\cdots,\x_N)$ sont des r\'ealisations ind\'ependantesd'une loi m\'elange :$$f(\x)=\sum_{k=1}^K p_k f_k(\x | \theta_k),$$alors la log-vraisemblance des param\`etres $\Phi=(p_1,\cdots,p_{K-1},\theta_1,\cdots,\theta_K)$s'exprime comme :$$L(\Phi ; \x_1,\cdots,\x_N)= \sum_{i=1}^{N} \log{ \left(p_k f_k(x_i | \theta_k)    \right )}.$$En g\'en\'eral l'algorithme EM (chapitre 3) est utilis\'e pour trouverdes estimateurs du m.v. (maximum de vraisemblance). Dans le cadre de cetteapproche, une partition des donn\'ees peut \^etre obtenue \`a partir des estimateur du m.v. en affectant chaque vecteur forme \`a la composante du m\'elange (donc la classe) la plus probable. La probabilit\'e conditionnelle que $\x_i$ soit issu de la $k$\ieme composante est donn\'ee par :\begin{equation}\label{eq:tkxi}t_k(\x_i)=\frac{\hat{p}_k f_k(x_i | \hat{\theta}_k) }{\sum_{\ell=1}^{K}\hat{p}_{\ell} f_{\ell}(x_i | \hat{\theta}_{\ell})}.\end{equation}\subsubsection{Approche classification}Une  autre approche possible, dans une optique de partitionnementde l'\'echantillon, consiste \`a consid\'erer directement la partition comme le param\`etre inconnu. Les pionniers de cette approche sont \citeasnoun{Scott1971}et \citeasnoun{Schroeder1976}. Dans ce contexte le probl\`eme \`a r\'esoudrepeut \^etre formul\'e comme suit : \'etant donn\'e un \'echantillonde taille $N$,  $(\x_1,...,\x_N)$, rechercher une partition dure$P=({\cal C}_1,\cdots,{\cal C}_K)$, K \'etant suppos\'e connu, telle que chaque classe ${\cal C}_k$ soit assimilable \`a un sous-\'echantillon suivantla loi $f_k(.|\theta_k)$.Le crit\`ere consid\'er\'e alors n'est plus la vraisemblance de l'\'echantillon, mais la vraisemblance classifiante, soit le produit des vraisemblance sur les classes. La log-vraisemblances'\'ecrit alors :\begin{equation}CML(\Phi ,\bc ; \x_1,\cdots,\x_N)=\sum_{k=1}^K \sum_{i=1}^n c_{ik }\log{ \{f_k(\x_i | \theta_k  \} }\end{equation}avec $\Phi=(\theta_1,\cdots,\theta_K)$ et $\bc=\{ c_{ik}\}$ une matrice de partition dure qui d\'efinit $K$ classes (ou sous \'echantillons). Ce crit\`ere est la log-vraisemblance associ\'ee \`a $K$ \'echantillonss\'epar\'es de taille fix\'ee.La vraisemblance classifiante ne fait pas appara\^{\i}tre explicitementla notion de proportions entre les diff\'erentes sous-populationset tend en pratique \`a produire despartitions o\`u les classes sont de tailles comparables. En faitle crit\`ere suppose implicitement que toutes les sous-populationssont de m\^eme taille. Cette limitationa incit\'e \citeasnoun{Symons1981} \`a p\'enaliser la vraisemblance classifiantepar un terme prenant en compte les proportions $(p_1,\cdots,p_K)$ desdiff\'erents sous-\'echantillons :\begin{equation}CML'(\Phi',\bc)=CML(\Phi, \bc)+ \sum_{k=1}^K n_k \log{p_k}\end{equation}o\`u $\Phi'=(\Phi,p_1,\cdots,p_K)$ et $n_k$ est l'effectif de la $k$\iemeclasse.Notons qu'en introduisant les variables $c_{ik}$ dans le terme de p\'enalit\'e,la vraisemblance classifiante p\'enalis\'ee s'\'ecrit\begin{eqnarray*}CML'(\Phi',\bc) & = & CML(\Phi, \bc ) + \sum_{k=1}^K \sum_{i=1}^N c_{ik} \log{p_k}\\             & = &\sum_{k=1}^K \sum_{i=1}^N c_{ik} \log{\{ p_k f_k( \x_i | \theta_k ) \} },\\             & = &\sum_{k=1}^K \sum_{i=1}^{n_k} \log  P(\x_{i,k}, k; \Phi'),\end{eqnarray*}o\`u $\x_{i,k}$ d\'enote un vecteur forme de la classe $k$. Ce dernier crit\`eres'interpr\`ete comme la log-vraisemblance d'un \'echantillon al\'eatoire$\{(\x_1,c(\x_1)),\cdots,(\x_N,c(\x_N))\}$ o\`u  \`a la diff\'erence d'un probl\`eme declassement les \'etiquettes $c(\x_i)$ n'ont pas \'et\'e observ\'ees. Ces deux crit\`eres peuvent \^etre maximis\'es par une version classificatoirede l'algorithme EM : {\em Classification EM algorithm}. L'algorithme CEMa \'et\'e propos\'e par \citeasnoun{Celeux1992a}. Une it\'eration cet algorithme se d\'ecompose ainsi :\begin{itemize}\item {\bf \'Etape E (estimation) :} Calcul des probabilit\'es $t_k(\x_i)^q$(cf. \'equation \ref{eq:tkxi}) pourchaque $\x_i$.\item {\bf \'Etape C (classification) :} Chaque $\x_i$ est affect\'e \`a la composante du m\'elange de plus forte probabilit\'e a posteriori. Une partition$P^{q+1}$ est donc d\'efinie caract\'eris\'ee par la matrice $\bc=\{ c_{ik} \}$ avec$c_{ik}=1$ si $k=arg \max_\ell{ t_\ell(x_i)^q}$ et $c_{ik}=0$ sinon.\item {\bf \'Etape M (maximisation) :} Calcul des estimateurs du m.v. de $\Phi^{q+1}$ sur la base des sous-\'echantillons pr\'ecis\'es par la matricede classification dure $\bc$.\end{itemize}L'algorithme CEM g\'en\`ere une suite $CML'(\Phi^q,\bc^q)$ croissante quiatteint son maximum en un nombre fini d'it\'erations \cite{Celeux1992a}.%Lien avec les centres mobiles.L'algorithme CEM est un algorithme tr\`es g\'en\'eral de classificationqui permet d'optimiser de nombreux crit\`eres de classification de type inertiels suivant les mod\`eles gaussiens consid\'er\'es.Prenons par exemple le mod\`ele gaussien le moins contraint, pour lequel les classes sont de tailles diff\'erentes et poss\`edent unematrice de variance covariance quelconque,l'algorithme CEM maximise alors le crit\`erede vraisemblance classifiante p\'enalis\'ee. Si toutes les proportionssont fix\'ees \'egales, le crit\`ere optimis\'e est alors simplementla vraisemblance classifiante. Un autre cas particulier int\'eressantest celui o\`u les densit\'es $f_k(.|\theta_k)$ du m\'elange sont desgaussiennes de vecteur moyenne $\bmu_k$ et de matrice de variance covariance$\bSigma_k=\lambda \cdot I$, m\'elang\'es en proportions \'egales. En effet le crit\`ere optimis\'e est la somme des variances intra-classes,\begin{eqnarray*} CML(\Phi,\bc ) & = &  \sum_{k=1}^K \sum_{i=1}^N c_{ik}                 \log{  (2\pi \det{|\bSigma_k|})^{-\frac{d}{2}}                 \exp{(-\frac{1}{2}(\x-\bmu_k)^t{(\lambda \cdot I)}^{-1} (\x-\bmu_k))}}\\             & = &  -\frac{1}{2 \lambda}\sum_{k=1}^K \sum_{i=1}^N c_{ik}                 (\x-\bmu_k)^t (\x-\bmu_k) + Cst,\\            & = & -\frac{1}{2 \lambda} trace(\bS_W) + Cst, \end{eqnarray*}et l'algorithme CEM,  avec ce mod\`ele, est exactement l'algorithme descentres mobiles pr\'esent\'e dans la section \ref{sec:algo}. Diff\'erentes \'etudes \cite{Celeux1992b} ont montr\'e que l'approche classificationintroduisait un biais dans l'estimation des param\`etres. En effet, cette approcheestime les param\`etres du m\'elange sur la base des classes, alors que les classessont disjointes et constituent en fait des \'echantillons tronqu\'es descomposantes du m\'elange. Ce ph\'enom\`ene a tendance \`a surestimer les diff\'erencesentre les moyennes, et \`a sous estimer les variances et les diff\'erences entre proportions. Ces inconv\'enients ne sont pas r\'edhibitoires si les classessont bien s\'epar\'ees et les proportions du m\^eme ordre de grandeur. \subsubsection{Liens avec la classification floue}%+++++++++++++++++++++++++++++++++++++++++++++++++++++++Dans le cadre de la reconnaissance des formes, l'algorithme EMpour les mod\`eles de m\'elanges peut \^etre interpr\'et\'e comme unalgorithme d'optimisation altern\'ee d'un certain crit\`ere \cite{Hathaway1986,Celeux1994}.Si les probabilit\'es a posteriori $t_k(\x_i)$ sont consid\'er\'ees comme des variables not\'ees $c_{ik}$, la log-vraisemblance $L(\Phi;\x)$ devient une fonction du vecteur $\Phi$ et des $c_{ik}$ que nous noterons :\begin{equation}L(\bc,\Phi)=\sum_{i=1}^N \sum_{k=1}^K c_{ik} \log{p_k f_k(\x_i | \theta_k)}           - \sum_{i=1}^N \sum_{k=1}^K c_{ik} \log{ c_{ik} },\end{equation} avec $\bc=\{c_{ik} :0 \leq c_{ik} \leq 1, \sum_{k=1}^K c_{ik}=1, \sum_{i=1}^Nc_{ik}>0 (1\leq i \leq N, 1 \leq k \leq K)   \}$.Consid\'erons le probl\`eme qui consiste \`a maximiser $L(\bc,\Phi)$ parrapport aux variables $\Phi$ et $\bc$. Il s'agit d'un probl\`emeclassique d'optimisation sous contraintes. Une m\'ethode d'optimisationpossible consiste \`a s\'eparer les variables en deux groupes et \`a optimiserle crit\`ere alternativement par rapport \`a un groupe en gardant fixe  les valeurs des variable de l'autre groupe. Dans le cas du crit\`ere $L(\bc,\Phi)$,pour la $q$\ieme it\'eration il est possible d'optimiser  alternativement par rapport \`a $\bc$ puis \`a $\Phi$ :\begin{enumerate}\item Maximisons $L(\bc,\Phi)$ par rapport \`a $\bc$ : le lagrangien s'\'ecrit\begin{equation}{\cal{L}}(\bc)=L(\bc,\Phi) + \sum_{i=1}^N \lambda_i (\sum_{k=1}^K (c_{ik}-1)),\end{equation}o\`u les $\lambda_i$ sont les coefficients de Lagrange correspondant aux contraintes $$\sum_{k=1}^K c_{ik}=1.$$ Les conditions n\'ecessaires d'optimalit\'e am\`enent les \'equations suivantes :\[\left \{ \begin{array}{l}	 				\frac{\partial{\cal{L}}}{\partial c_{ik}}=\log{(p_kf_k(\x_i|\theta_k))} -1 -\log{c_{ik}} + \lambda_i = 0 ; \\				  \sum_{k=1}^K c_{ik}=1 ;	               \end{array}     		\right .\]ce qui donne,\[\left \{ \begin{array}{l}	 				c_{ik}=\exp{\{ \log(p_kf_k(\x_i|\theta_k)) -1 + \lambda_i \}};  \\				  \sum_{k=1}^K \exp{\{ \log(p_kf_k(\x_i|\theta_k)) -1 + \lambda_i \}} =1;	          \end{array}     		\right .\]Ainsi les nouvelles valeurs des $c_{ik}$ sont :\begin{equation}c_{ik}=\frac{p_k f_k(\x_i | \theta_k)}{f(\x_i)}.\end{equation}\item La maximisation de  $L(\bc,\Phi)$ par rapport \`a $\Phi$ est \'equivalente l'\'etape M de l'algorithme EM.  \end{enumerate}Ces deux \'etapes qui visent \`a maximiser le crit\`ere $L(\bc,\Phi)$ sontidentiques aux deux \'etapes de l'algorithme EM appliqu\'e \`a un m\'elangede distribution de probabilit\'e.Si l'on consid\`ere $\bc$ comme une matrice de classification floue (elle ena toutes les caract\'eristiques), l'algorithme EM peut \^etre interpr\'et\'ecomme un algorithme de classification floue. Remarquons que le crit\`ere optimis\'e s'\'ecrit comme la somme de deux termes :\begin{itemize}\item Dans la terminologie utilis\'ee en classification automatique,le premier est appel\'e ``vraisemblance classifiante floue'' (avec proportionslibres). Plusieurs algorithmes de classification automatique existent qui visent \`a trouver la partitiondure qui optimise la vraisemblance classifiante. \item Le second terme peut \^etre consid\'er\'e comme une entropie, ou encore une mesure de floue de la partition. Ce second terme est maximum si la partitionobtenue est compl\`etement floue et minimum (nul en l'occurence) $\bc$ est une matrice de partition dure. \end{itemize}Au vu des remarques pr\'ec\'edentes, l'algorithme EM peut \^etre consid\'er\'ecomme un algorithme de classification flou qui optimise uncrit\`ere de classification p\'enalis\'e par une entropie. \section{Hi\'erarchies}\begin{defi}${\cal F}$ \'etant un ensemble fini,un ensemble $H$ de parties non vides de ${\cal F}$ est une hi\'erarchie si :\begin{enumerate}\item ${\cal F} \in H$\item $\forall x \in {\cal F}$, $\{ x \} \in H$ \item $\forall h,h' \in H$,$h \cap h'=\emptyset$ ou $h\subset h'$ ou $h'\subset h$\end{enumerate}\end{defi}Une hi\'erarchie peut \^etre vue comme un ensemble de partitions embo\^\i t\'ees. Graphiquement une hi\'erarchie est souvent repr\'esent\'eepar une structure arborescente appel\'ee  dendogramme. \begin{ex}Exemple de hi\'erarchie :en biologie les diff\'erente races d'animaux sont regroup\'ees en esp\`eces, qui sont ellesm\^eme regroup\'ees en grande famille...  \end{ex}Une hi\'erarchie peut \^etre obtenue par deux types de m\'ethodes, selon que l'arbre est construit en commen\c cant par les feuilles ou bien la racine :\begin{itemize}\item la classification ascendante (``agglom\'erative'') consid\`ereinitialement chaque vecteur forme comme une classe. \`A chaqueit\'eration les deux classes les plus proches sont agr\'eg\'eespour former une nouvelle classe. Le processus se termine naturellementlorsqu'il ne reste qu'une seule classe.\item la classification descendante (``divisive'' en anglais), part d'une seule classe (l'ensemble des vecteurs forme)partage celle-ci en deux. L'op\'eration est r\'ep\'et\'ee\`a chaque it\'eration jusqu`a ce toutes les classes obtenuescontiennent un unique vecteur forme. \end{itemize}             Il existe un parall\`ele int\'eressant entre la notion de distance ultram\'etrique et la notionde hi\'erarchie. Une distance utlram\'etrique $\delta$ v\'erifie toutes les propri\'et\'es qui d\'efinissent une distance classique et satisfait en plus l'in\'egalit\'e$$\delta(\x,\z) \leq \max \left ( \delta(\x,\z),\delta(\z,\y) \right ),$$plus forte que l'in\'egalit\'e triangulaire. Lorsqu'on dispose d'une hi\'erarchie, on peutinterpr\'eter le nombre minimum d'emboitements n\'ecessaires pour que deux vecteurs formeappartiennent \`a une m\^eme classe, comme une dissimilarit\'e. Il est alors possible de montrer que cette dissimilarit\'e est une distance ultram\'etrique.Ainsi, il est possible d'intepr\'eter le probl\`emede la classification hi\'erarchique comme la recherche d'une ultram\'etrique $\delta$  prochede $d$, la dissimilarit\'e  utilis\'ee sur ${\cal F}$ l'ensemble \`a classer.    \subsection{Classification ascendante hi\'erarchique}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Le principe des algorithmes de classification hi\'erarchiqueascendante est tr\`es simple :\begin{description}\item[Initialisation :] chaque \'el\'ement de ${\cal F}$ constitueune classe. Une ``distance'' $D$ est calcul\'ee entre toutes lesclasses. \item[Tant que] nombre de classes $>$ 1\begin{itemize}\item regrouper les deux classes les plus proches au sens de la ``distance''$D$,\itemcalcul des ``distances'' entre la nouvelle classe et les autres.\end{itemize} \end{description}La ``distance'' $D$ entre deux partie $h$ et $h'$ de ${\cal F}$, peut\^etre d\'efinie de nombreuses mani\`eres \`a partir d'unemesure de dissimilarit'e $d$ sur ${\cal F}$. \subsubsection{Crit\`eres d'agr\'egation}%------------------------------------------------------La ``distance'' $D$ est couramment appel\'ee crit\`ere d'aggr\'egation.Quatre variantes sont principalement utilis\'ees :\begin{itemize}\itemle crit\`ere du lien minimum (``single link'')  :$$D(h,h')= \min \left [     d(\x , \y) / \x \in h \ et \ \y \in h' \right ],$$\itemle crit\`ere du lien maximum (``complete link'')  :$$D(h,h')= \max \left [     d(\x , \y) / \x \in h \ et \ \y \in h' \right ],$$\itemle crit\`ere de la distance moyenne (``group average'')  :$$D(h,h')= \frac{\sum_{i=1}^{n_{h}}  \sum_{j=1}^{n_{h'}}  d(\x_i , \x_j)}{n_{h} \cdot n_{h'}}, $$\itemle crit\`ere de \citeasnoun{Ward1963}$$D(h,h')= \frac{n_{h} \cdot n_{h'}}{n_{h} + n_{h'}}\|\bm_{h} - \bm_{h'} \|^2.$$\end{itemize}\subsubsection{Crit\`ere d'inertie intra classe et m\'ethode de Ward}%------------------------------------------------------Lorsque l'on dispose d'une partition en $K$ classe, le crit\`ered'inertie intra-classe mesure son homog\'en\'eit\'e :\begin{eqnarray*}I_W  & = & trace(\bS_W),\\     & = & \sum_{k=1}^K \sum_{i=1}^{n_k} (\x_{ik} - \bm_k )^t (\x_{ik} - \bm_k).\end{eqnarray*} Consid\'erons deux partitions :\begin{itemize}\item $P=\left ( {\cal C}_1, \cdots , {\cal C}_K  \right)$,\item et $P'$, la partition obtenue en fusionnant les classes ${\cal C}_k$ et ${\cal C}_{\ell}$. \end{itemize}On peut montrer que la diff\'erence entre l'inertie des deux partitionsest \'egale au crit\`ere d'aggr\'egation de Ward :$$I_{W'}-I_W = \frac{n_{k} \cdot n_{\ell}}{n_{k} + n_{\ell}}\|\bm_{k} - \bm_{\ell} \|^2.$$Ainsi,\`a chaque \'etape de l'algorithme de Ward choisit unenouvelle partition qui limite l'augmentation de l'inertie intra-classe.Notons que cette propri\'et\'e ne garantie pasl'optimisation globale du crit\`ere.\subsection{Classification descendante hi\'erarchique}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%La classification descendante est beaucoup moins populaire que lesm\'ethodes d\'ecrites pr\'ec\'edemment. En th\'eorie, la premi\`ere\'etape d'une m\'ethode descendante doit comparer les $2^{N-1}-1$ partitionspossibles des $N$ vecteurs forme, en deux classes. Pour \'eviter des calculs impossibles, une solution consiste \`a appliquer une m\'ethode de partitionnement pour obtenir les deux classes. En r\'ep\'etantce processus r\'ecursivement sur chaque classe obtenue, il enr\'esulte une hi\'erarchie.   