\begin{eqnarray*}\hat{c}(\x) & = & arg \max_k \pi (k | \x ),\\            & = & arg \max_k p_k \cdot f_k(\x),\\            & = & arg \min_k -2 \cdot \log p_k  + (\x - \bmu_k)^t \bSigma^{-1} (\x - \bmu_k).\end{eqnarray*}Remarquons que si les proportions $p_k$ sont toutes \'egales, alors cette r\`egle ded\'ecision revient \`a affecter un vecteur forme \`a la classe la plus proche ausens de la distance de \citeasnoun{Mahalonobis1936}$$\delta(\x,\bmu_k)=[(\x - \bmu_k)^t \bSigma^{-1} (\x - \bmu_k)]^{\frac{1}{2}}.$$ Si, de plus, la matrice de variance est proportionnelle \`a la matrice identit\'e,$$\bSigma=\sigma \cdot I,$$alors la distance de Mahalanobis est \'equivalente \`a la distance euclidienne. Dans ce derniercas les classes sont suppos\'ees avoir une forme sph\'erique et un volume $\sigma$.Lorsque les proportions sont diff\'erentes, le terme $-2 \cdot \log p_k$ biais la d\'ecision en faveur de la classes la plus probable {\em a priori}.La r\`egle de d\'ecision peut s'exprimer sous une forme plus simple lorsque le termequadratique est d\'evelopp\'e, car $\x^t \bSigma^{-1} \x$ est une expression ind\'ependantede l'indice de classe : \begin{eqnarray*}\hat{c}(\x) & = & arg \min_k (\bSigma^{-1} \bmu_k)^t \cdot \x + (\frac{1}{2}{\bmu_k}^t\bSigma^{-1} \bmu_k + \log p_k),\\            & = & arg \min_k \bw_k^t \cdot \x + w_{k0}.\end{eqnarray*}La fonction de d\'ecision est lin\'eaire et on parle d'analyse discriminante lin\'eaire, ce quiimplique, que les fronti\`eres s\'eparant deux  r\'egions voisines de d\'ecision, sont des hyperplans.Consid\'erons  ${\cal R}_k$ et ${\cal R}_\ell$ deux r\'egions contigues :  la fronti\`ere entre cesdeux r\'egions est d\'ecrite par l'\'equation : $$\bSigma^{-1} (\bmu_k-\bmu_\ell)\cdot (\x - \x_0)=0,$$ o\`u $$\x_0=\frac{1}{2}(\bmu_k - \bmu_ell) - \log \frac{p_k}{p_\ell} \frac{(\bmu_k-\bmu_\ell)}{(\bmu_k-\bmu_\ell)^t\bSigma^{-1} (\bmu_k-\bmu_\ell }.$$Ainsi la surface s\'eparatrice est un hyperplan orthogonal \`a $\bSigma^{-1} (\bmu_k-\bmu_\ell)$ etpassant par le point $\x_0$.Notons que dans le cas particulier o\`u les proportions sont \'egales et la matrice de variance proportionnelle \`a la matrice identit\'e, alorsl'hyperplan est orthogonal \`a l'axes reliant les vecteur moyennes $\bmu_k$ et $\bmu_\ell$et le point $\x_0$ est exactement au milieu du segment d\'efini par $\bmu_k$ et $\bmu_\ell$.Si les proportions sont diff\'erentes cela revient \`a translater l'hyperplan vers laclasse la moins probable.\begin{ex}\cite{Ripley1996}(suite de l'exemple \ref{ex:erreur})Probabilit\'e d'erreur dans le cas de deux classes gaussiennes sous hypoth\`ese d'homosc\'edasticit\'e :la r\`egle de d\'ecision prend la forme suivante$$\hat{c}(\x)=\left \{ \begin{array}{l}1 \ si \ A=(\bmu_1 - \bmu_2)^t \bSigma^{-1}(\x - \frac{1}{2}(\bmu_1 + \bmu_2)) > \log \frac{p_1}{p_2} , \\2 \ sinon.\\\end{array}\right .$$Si $X$ appartient \`a la classe 1 alors on peut montrer que$$A \sim {\cal N}(\frac{1}{2}\delta^2,\delta^2)$$avec $\delta=[(\bmu_1 - \bmu_2)^t \bSigma^{-1} (\bmu_1 - \bmu_2)]^{\frac{1}{2}}$.De m\^eme si $X$ appartient \`a la seconde classe alors $$A \sim {\cal N}(-\frac{1}{2}\delta^2,\delta^2)$$Maintenant,  la probabilit\'e d'erreur peut s'\'ecrire comme : \begin{eqnarray*}P(\mbox{Erreur})& = &  P(\X \in {\cal R}_2 | C=1) \cdot p_1 +  			    P(\X \in {\cal R}_1 | C=2) \cdot p_2,\\		& = &  p_1 \cdot P(A\leq \log \frac{p_1}{p_2}| C=1) + p_2 \cdot  P(A > \log \frac{p_1}{p_2}| C=2),\\                & = &  p_1 \cdot \Phi(-\frac{1}{2} \delta + \frac{1}{\delta} \log \frac{p_1}{p_2}) + p_2 \cdot \Phi(-\frac{1}{2} \delta - \frac{1}{\delta} \log \frac{p_1}{p_2})\end{eqnarray*}\end{ex}