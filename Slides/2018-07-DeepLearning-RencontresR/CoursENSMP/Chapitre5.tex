\nocite{Gallinari1988}\chapter{Extraction et s\'election de caract\'eristiques}En reconnaissance des formes, les algorithmes de classement classent des vecteurs forme $\x_i$ qui sont des descriptions d'objets. Cette approche fait l'hypoth\`ese qu'une description coh\'erente de l'ensemble des objets existe. Dans ce contexte, laquestion suivante \'emerge : comment choisir les caract\'eristiquesdescriptives. Ce chapitre envisage quelques r\'eponses \`a cetteinterrogation.Dans un premier temps, pr\'ecisons le probl\`eme. Si l'on suppose que chaque objet est caract\'eris\'e par $d$ variablesquantitatives (poids, taille, vitesse...), et appartient\`a une classe parmi $K$,  le probl\`eme pr\'ec\'edent peut alors se reformuler comme suit :\begin{itemize}\item quelles variables parmi les $d$ disponibles sont les plus discriminantes ?\item comment extraire de nouvelles variables, \`a partir desvariables initiales, qui soient discriminantes ? \end{itemize}Utilis\'ees comme pr\'etraitement des donn\'ees, ces deux approches, extraction  et s\'election de caract\'eristiques,permettent de faciliter la t\^ache du classifieur. La r\'eductionde dimension (passage de $d$ \`a $q$ variables avec $q<d$) estpar exemple d'une grande aide pour les m\'ethodes non param\'etriquesde classement bas\'ees sur les estimateurs \`a noyaux (chapitre 3). Eneffet, ces derni\`eres m\'ethodes sont inefficaces dans les espacesde grandes dimensions et gagnent \`a \^etre utilis\'ees apr\`es r\'eduction de dimension. Dans le cadre d'une analyse exploratoire  des donn\'ees, les techniquesd'extraction et de s\'election de caract\'eristiques permettent ausside visualiser les vecteurs forme. Dans ce chapitre nous pr\'esentons :\begin{itemize}\item l'analyse factorielle discriminante, qui est une techniqued'extraction de caract\'eristiques cherchant des combinaisons lin\'eairesdes variables initiales ;\item quelques m\'ethodes de positionnement multidimensionel (``multidimensional scaling''),qui ne sont pas sp\'ecifique au probl\`eme du classement, mais permettentla visualisation des vecteurs forme ;\item les principales  m\'ethodes de s\'election de caract\'eristiques.\end{itemize} \section{Analyse factorielle discriminante}% historique et but de la m\'ethode\L'analyse factorielle discriminante vise \`a trouver des nouvellesvariables, appel\'ees facteurs, combinaisons lin\'eaires des variables initiales, qui permettent de distinguer le mieux possible, les $K$ groupes de vecteurs forme. Historiquement la m\'ethodea \'et\'e introduite par \citeasnoun{Fisher1936} dans le cas de deuxclasses. La g\'en\'eralisation au cas multi-classes est due \`a \citeasnoun{Rao1948}Pr\'ecisons quelques notations utiles \`a  la pr\'esentationde la m\'ethode :\begin{itemize}\itemles vecteurs moyennes des $K$ groupes sont not\'es$$\bm_k=\frac{1}{n_k}\sum_{i=1}^{n_k} \x_{i,k}.$$\item$\bm$ d\'enote le vecteur moyenne de l'ensemble des vecteurs formes, $$\bm= \frac{1}{N}\sum_{k=1}^{n_k} n_k \cdot \bm_k,$$\itemla matrice d'inertie intra-classe $\bS_W$ ($W$ comme ``within'') est d\'efinie par :\begin{eqnarray*}\bS_W &=&  \sum_{k=1}^K \sum_{i=1}^{n_k} (\x_{i,k}-\bm_k)(\x_{i,k}-\bm_k)^t,\end{eqnarray*}\itemla matrice d'inertie inter-classe $\bS_B$ ($B$ comme ``between'') est d\'efinie par :\begin{eqnarray*}\bS_B & = & \sum_{k=1}^K n_k \cdot (\bm_k - \bm)(\bm_k - \bm)^t ,\end{eqnarray*}\itemet enfin la matrice d'inertie totale $\bS_T$ s'\'ecrit :\begin{eqnarray*}\bS_T &=& \sum_{k=1}^K \sum_{i=1}^{n_k} (\x_{i,k}-\bm) (\x_{i,k}-\bm)^t \\    &=& \sum_{k=1}^K \sum_{i=1}^{n_k} (\x_{i,k}-\bm_k+\bm_k-\bm)(\x_{i,k}-\bm_k+\bm_k-\bm)^t\\    &=&     \sum_{k=1}^K \sum_{i=1}^{n_k} (\x_{i,k}-\bm_k)(\x_{i,k}-\bm_k)^t +           \sum_{k=1}^K n_k \cdot (\bm_k - \bm)(\bm_k - \bm)^t\\   &=& \bS_W + \bS_B.\end{eqnarray*}Notons que cette derni\`ere \'egalit\'e est une application du th\'eor\`eme deHuygens et  une g\'en\'eralisation de la formule de l'analyse de la variance.\end{itemize}\subsection{Discriminant lin\'eaire de Fisher}Supposons un ensemble d'apprentissage ${\cal F}$  compos\'e de $n_1$ vecteurs forme $\x_{i,1}$ de la classe${\cal C}_1$ et $n_2$ vecteurs forme $\x_{j,2}$ de la classe${\cal C}_2$. Une combinaison lin\'eaire $y$ d'un vecteur $\x$$$y=\bw^t \cdot \x,$$est un scalaire. Dans le but de poser un probl\`eme poss\'edant unesolution unique, on impose $\| \bw \|=1$.Le but de l'analyse discriminante de Fisherest de trouver un facteur $\bw$ tel que lessur laquelle lesprojections des vecteurs $\x_{i,1}$ soit bien s\'epar\'ees des projections des vecteurs $\x_{j,2}$.Notons :\begin{itemize}\item $y_{i,k}$ les projections des $\x_{i,k}$ : $y_{i,k}= \bw^t \cdot \x_{i,k}$,\item$\tilde{m}_k$ les projections des vecteurs moyennes :  $\tilde{m}_k= \bw^t \cdot \bm_{k}$,\item et $\tilde{s}_k^2$ l'inertie des projections des vecteurs formesdu groupe $k$ :$$\tilde{s}_k^2= \sum_{i=1}^{n_k} (y_{i,k}-\tilde{m}_k)^2.$$\end{itemize}Le crit\`ere mesurant la qualit\'e de cette s\'eparation surl'axe $\bw$, propos\'e par Fisher, est le rapport del'inertie inter-classes sur l'inertie intra-classes des projections des vecteurs forme de ${\cal F}$ :$$J(\bw)=\frac{(\tilde{m}_1 -\tilde{m}_2)^2}{\tilde{s}_1^2+\tilde{s}_2^2}.$$Intuitivement les deux groupe projet\'es seront bien s\'epar\'es,si l'\'ecart entre $\tilde{m}_1$ et  $\tilde{m}_2$ est grand, c'est-\`a-diresi les centre de gravit\'e des groupes sont les plus \'eloign\'es possiblesen projection, et si chaque classe projet\'ee forme le groupe le pluscompact possible autour de leur centre de gravit\'e respectif(inertie intra-classe petite). En d'autrestermes, la s\'eparation sera d'autant plus nette que le crit\`ere$J(\bw)$ sera important. Dans le contexte des tests d'hypoth\`eses, lorsque l'on d\'esiretester l'\'egalit\'e des moyennes  $\tilde{m}_1$ et  $\tilde{m}_2$ de deux groupes, l'analysede la variance am\`ene \`a d\'efinir la r\'egion critique suivante :$$J=\frac{(\tilde{m}_1- \tilde{m}_2)^2}{\tilde{s}_1^2+\tilde{s}_2^2} > A.$$On peut montrer que $J$ est proportionnelle \`a une variable al\'eatoirequi suit une loi de Fisher Snedecor ${F}_{K-1,N-K}$. La maximisationdu crit\`ere $J(\bw)$ revient donc \`a trouver un sous espace de projectiono\`u le test d'\'egalit\'e des moyennes sera le plus pessimiste possible.La recherche du vecteur $\bw$ peut donc \^etre formul\'e comme unprobl\`eme d'optimisation sous contrainte :$$\left \{ \begin{array}{l}\bw = arg \max_{\boldv}  J(\boldv)  \\\| \bw \|=1    \\\end{array}\right.$$ En faisant appara\^{\i}tre dans le crit\`ere $J(\bw)$ les vecteurs forme $\x_{i,k}$,il vient$$\left \{ \begin{array}{l}\bw = arg \max_{\boldv} \frac{\boldv^t(\bm_1-\bm_2)(\bm_1-\bm_2)^t \boldv}{\boldv^t \bS_W \boldv}  \\\| \bw \|=1    \\\end{array}\right.$$ Remarquons que \begin{equation}\label{eq:SBFisher}\bS_B= \frac{n_1 n_2}{N^2} (\bm_1-\bm_2)(\bm_1-\bm_2)^t.\end{equation}En effet si l'on consid\`ere que les vecteurs forme sont centr\'es ($\bm=0$),on a $$\bm=\frac{n_1}{N}\bm_1 +  \frac{n_2}{N}\bm_2=0.$$On peut donc \'ecrire que $$\left \{ \begin{array}{l}\bS_B = \frac{n_1}{N} \bm_1 \bm_1^t +  \frac{n_2}{N} \bm_2 \bm_2^t -        \underbrace{\left ( \frac{n_1}{N} \bm_1 \bm_1^t +  \frac{n_2}{N} \bm_2 \bm_1^t    \right )}_0 = -\frac{n_2}{N}\bm_2 (\bm_1 - \bm_2)^t \\\\\bS_B = \frac{n_1}{N} \bm_1 \bm_1^t +  \frac{n_2}{N} \bm_2 \bm_2^t -        \underbrace{\left ( \frac{n_1}{N} \bm_1 \bm_2^t +  \frac{n_2}{N} \bm_2 \bm_2^t    \right )}_0 = \frac{n_1}{N}\bm_1 (\bm_1 - \bm_2)^t  \\ \end{array}\right.$$En moyennant ($n_1$ fois la premi\`ere expression plus $n_2$ fois la deuxi\`eme) on montre l'\'equation \ref{eq:SBFisher} qui nous permet de poserle probl\`eme de la recherche de $\bw$ sous la forme : $$\left \{ \begin{array}{l}\bw = arg \max_{\boldv} \frac{\boldv^t \bS_B  \boldv}{\boldv^t \bS_W \boldv}  \\\| \bw \|=1    \\\end{array}\right.$$ On voit dans que $\lambda_{max}$ la valeur maximum du crit\`ere v\'erifie :\begin{eqnarray*}\lambda_{max} \cdot \bw^t \bS_W \bw & = &  \bw^t \bS_B \bw \\\lambda_{max} \cdot \bS_W \bw& = &  \bS_B \bw\end{eqnarray*}Si la matrice $\bS_W$ est inversible, r\'esoudre ce probl\`eme revient \`a chercher $\bw$ le vecteur propreassoci\'e \`a $\lambda_{max}$ la plus grande valeur propre de la matrice $\bS_W^{-1}\bS_B$ :$$ \lambda_{max} \cdot \bw = \bS_W^{-1}\bS_B \cdot \bw .$$Remarquons que la matrice $\bS_B$ est de rang 1, ce qui impliquequ'il n'existe qu'une seule valeur propre. Dans le cas qui nousint\'eresse on peut ais\'ement obtenir le vecteur propre en remarquantque $S_B\cdot \bw$ est toujours dans la direction du vecteur $\bm_1-\bm_2$:$$\bw= \alpha  \bS_W^{-1} (\bm_1-\bm_2).$$avec $\alpha$ un scalaire tel que $\|\bw\|=1.$\subsection{Cas g\'en\'eral}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%La g\'en\'eralisation du discriminant lin\'eaire de Fisher au cas multi-classesrecherche un espace vectoriel de dimension $p$ avec  $p<K-1$ telle que les$K$ classes soient s\'epar\'es au mieux dans cet espace.Le but de l'analyse factorielle discriminante consiste donc \`a trouver $p$vecteurs $\bw_k$, qui d\'efinissent $p$ variables discriminantes :$$y_k= \bw_k \cdot \x,$$L'ensemble des vecteurs $\bw_k$, les facteurs, peut s'\'ecrire de mani\`erecondens\'e sous la forme d'une matrice   $\bW$ de dimension $d$ par $p$ (chaque colonne de cette matrice  correspond \`a un des facteurs). Notons :\begin{itemize}\item $$\tilde{\bm}_k = \frac{1}{n_k} \sum_{i=1}{n_k} \y_{ik}              = \frac{1}{n_k} \sum_{i=1}{n_k} \y_{ik}=\bW^t \bm_k,$$les vecteurs moyennes des nouvelles variables ;\item$$\tilde{\bm}  =\bW^t \bm,$$le vecteur moyenne de l'ensemble des vecteurs forme, exprim\'e en fonction des facteurs ;\item\begin{eqnarray*}\tilde{\bS}_W &=& \sum_{k=1}^K \sum_{i=1}^{n_k} (\y_{ik}-\tilde{\bm}_k)(\y_{ik}-\tilde{\bm}_k)^t\\             & =& \bW^t \bS_W \bW,\end{eqnarray*}la matrice de d'inertie intra-classes des nouvelles variables ;\item\begin{eqnarray*}\tilde{\bS}_B & = & \sum_{k=1}^K n_k \cdot (\tilde{\bm}_k - \tilde{\bm})(\tilde{\bm}_k - \tilde{\bm})^t ,\\      &= &   \bW^t \bS_B \bW,\end{eqnarray*}la matrice d'inertie inter-classes des nouvelles variables.\end{itemize}Pour obtenir les vecteur $\bw_k$ composant la matrice $\bW$, il est possiblede proc\'eder par construction. Le vecteur $\bw_1$, appel\'e premier facteurdiscriminant, est la solution du probl\`eme :$$\left \{ \begin{array}{l}\bw_1 = arg \max_{\boldv} \frac{\boldv^t \bS_B  \boldv}{\boldv^t \bS_W \boldv}  \\\| \bw_1 \|=1    \\\end{array}\right.$$ Dans la section pr\'ec\'edente nous avons vu que la solution de ce probl\`eme\'etait le vecteur propre de $\bS_W^{-1}\bS_B$ associ\'e \`a la plusgrande valeur propre. Le second facteur, s'il existe, est d\'efini comme le vecteur propre associ\'e \`a la deuxi\`eme plus grande valeur propre.En poursuivant cette construction, on trouve autant d'axes discriminantsque de valeurs propres non nulles,  c'est \`a dire au plus $K-1$, lerang maximum de la matrice $\bS_W^{-1}\bS_B$. Remarquons que les vecteurspropres ainsi trouv\'es sont $\bS_W$ orthogonaux :$$\bw_\ell^t \cdot \bS_W \cdot \bw_k = 0$$si $\bw_k$ et $\bw_\ell$ sont deux facteurs distincts.Il semble aussi int\'eressant de savoir quel crit\`ereoptimise ce proc\'ed\'e de construction. \`A chaque \'etape,on cherche un vecteur $\bw_k$ diff\'erent de tous  les vecteurs trouv\'espr\'ec\'edemment, qui maximise le crit\`ere \begin{eqnarray*}J_k & = & \frac{\bw_k^t \bS_B  \bw_k}{\bw_k^t \bS_W \bw_k} \\    & = & \bw_k^t  \bS_W^{-1}\bS_B \bw_k \\    & = & trace \left [ \bw_k  \bw_k^t  \bS_W^{-1}\bS_B  \right ]\end{eqnarray*} Ainsi \`a la fin de la construction, on peut affirmer que lasomme des crit\`eres $J_k$ est maximis\'ee (car chacun des termes dela somme est maximis\'ee)  sous la contrainteque les vecteurs $\bw_k$ sont orthogonaux entre eux. La matrice$\bW$ est donc solution de :$$\left \{ \begin{array}{l}\max \sum_{k=1}^p  trace \left [ \bw_k  \bw_k^t  \bS_W^{-1}\bS_B  \right ] = \max trace \left [ \bW^t \bS_W^{-1}\bS_B \bW \right ]         \\\bW^t \cdot \bS_W \cdot \bW=I\end{array}\right.$$Ce crit\`ere met en \'evidence les liens existant avec l'analyse en composantesprincipale.Remarquons qu'il est possible de montrer que la matrice $\bW$ est aussisolution de $$\left \{ \begin{array}{l}\max \frac{det\left ( \tilde{\bS}_B \right )}{det \left (\tilde{\bS}_W \right ) }=\frac{det\left ( \bW^t \bS_B \bW \right )}{det\left (\bW^t \bS_W \bW \right )}\\\bW^t \bS_W \bW=I\end{array}\right.$$Ce dernier crit\`ere peut  s'interpr\`eter comme le rapport entre deux volumes. Ce rapport sera important lorsque  les classes projet\'ees  occuperons en moyenne un petit volume autourde leur moyenne ($det(\tilde{\bS}_W)$ petit), et que les vecteurs moyennesprojet\'es occuperons occuperons un grand volume par rapport au pr\'ec\'edent.Ceci correspond intuitivement bien \`a la notion de s\'eparation que l'on recherche.Cette formulation du probl\`eme en terme de crit\`ere globale ale d\'efaut d'admettre plusieurs solutions $\bW$ dont la solutionobtenue par construction. En effet toutes les transformation lin\'eairesnon singuli\`ere de $\bW$ laisse les crit\`eres invariants.Notons que la matrice $\bW$ peut \^etre trouver de deux autre mani\`eres. Il est en effet direct de montrer que :$$arg \max_{\bw} \frac{\bw^t \bS_B \bw}{\bw^t \bS_W \bw} =arg \max_{\bw} \frac{\bw^t \bS_T \bw}{\bw^t \bS_W \bw} =arg \max_{\bw} \frac{\bw^t \bS_B \bw}{\bw^t \bS_T \bw}.$$En utilisant le processus de construction pr\'ec\'edent \`a partirde ces crit\`eres, on trouve la m\^eme matrice $\bW$. Il s'ensuitque les matrices  $(\bS_W^{-1}\cdot \bS_B)$, $(\bS_W^{-1}\cdot \bS_T)$ et $(\bS_T^{-1}\cdot \bS_B)$ ont m\^eme vecteurs propres.%\subsection{Analyse en composante principale et analyse factorielle discriminante}%L'analyse factorielle discriminante (AFD) peut s'interpr\`eter comme %une analyse en composante%principale particuli\`ere. Il existe de nombreuses fa\c{c}on%de pr\'esenter l'analyse en composantes principales (ACP). Rappelons bri\`evement%l'une d'elle :%soit un ensemble (dans notre cas $\R^d$) muni d'une m\'etrique%$M$. L'inertie totale d'un nuage de point $\x_1,\cdots,\x_N$ est %d\'efinie par :%\begin{equation}%\label{eq:idem}%I_T=\sum_{i=1}^N (\x_i-\bm)M(\x_i-\bm)^t=trace\left[ M \sum_{i=1}^N (\x_i-\bm)^t(\x_i-\bm)   \right]=trace \left [ M \bS_T  \right ] %\end{equation}%L'analyse en composante principale  peut se d\'efinir comme la recherche d'un%ensemble de facteurs, d\'efinissant de nouvelles variables, qui poss\`edent%une variance maximum et qui sont orthogonaux par rapport \`a une m\'etrique %$M^{-1}$. %d'une sous espace vectoriel, sur lequel l'inertie projet\'ee%soit maximum. Cela revient \`a chercher une matrice de projection%$\bW$ telle que :%$$%\left \{ \begin{array}{l}%\bW = arg \max_{\bV}trace \left [ \bV^t \cdot M \bS_T  \cdot \bV \right ] \\%\bW^t \bW=I%\end{array}%\right.%$$%La solution bien connue de ce probl\`eme est %$\bW$ la matrice des vecteurs propres de $M \bS_T$. %Cette pr\'esentation permet de voir que l'analyse factorielle%discriminante est une ACP de l'ensemble des vecteurs forme,%r\'ealis\'ee avec la m\'etrique%$\bS_W^{-1}$.%D'apr\`es les \'egalit\'es \ref{eq:idem}, on d\'eduit que %l'AFD peut aussi s'interpr\`eter comme une ACP des%vecteurs moyennes $\bm_1,\cdots,\bm_K$ avec la m\'etrique%$\bS_T^{-1}$ ou $\bS_W^{-1}$. %L'impl\'ementation classique de l'AFD utilise cette derni\`ere relation :%\begin{itemize}%\item%les vecteurs formes initiaux sont transform\'ees de mani\`eres \`a ce%que les nouveaux vecteurs poss\`edent une matrice d'inertie %intra classe $\bS_W$ \'egale \`a l'identit\'e, %\item%puis la matrice d'inertie inter-classe, calcul\'ee \`a partir des%vecteurs transform\'es, est d\'ecompos\'ee sur sa base de vecteurs propres.%\end{itemize}%Notons enfin, que Gallinari {\em et al.} (1988) on  montr\'e que%l'AFD entretient des relations \'etroites%avec les r\'eseaux de neurones multi-couches poss\'edant une couche %cach\'ee et des fonctions d'activation lin\'eaires.\section{Multidimensional scaling}Pour r\'eduire la dimension des vecteurs forme, une alternativeconsiste \`a utiliser des techniques demultidimensionnal scaling. Ces techniques sont tr\`esutilis\'ees en psychologie et sociologie, o\`u les objetsd'\'etude (les individus) ne sont pas d\'ecrit en terme decaract\'eristiques individuelles mais les uns par rapport aux autres par la mesure de leur diff\'erence deux \`a deux. Ces  mesures sont appel\'ees dissimilarit\'es et non distances car elles ne v\'erifient pas l'in\'egalit\'e triangulaire. Les m\'ethodes de  multidimensional scaling visent\`a repr\'esenter au mieux des objets dans un espace visualisable, de fa\c{c}on \`a ce que les distancesentre ces objets dans cet espace soient aussi proches que possible des dissimilarit\'esinitiales.Les dissimilarit\'es mesurent les diff\'erences entre toutesles paires d'objets. Ainsi les dissimilarit\'es entre $N$ objetssont sp\'ecifi\'ees par une matrice $N \times N$,$\delta=\{ \delta_{ij}\}_{i,j=1..N}$. Ce genre de matricespeut avoir  diff\'erentes origines    :\begin{itemize}\item Le jugement humain peut souvent \^etre traduit par desmesures de  dissimilarit\'es. Une personne amen\'ee \`a quantifierla diff\'erence de confort entre deux voitures  pourra, parexemple, choisir une chiffre entre 1 et 10 qui correspond \`a l'intensit\'e de la diff\'erence per\c{c}ue.\item Les donn\'ees telles que les temps de transports entredes paires de villes se pr\'esentent naturellement sous la formed'une matrice de dissimilarit\'es.\item Enfin, notons qu'il est toujours possible de d\'eriverune matrice de dissimilarit\'es (les distances sont des dissimilarit\'es) d'un ensemble de vecteursforme. Il suffit en effet de calculer les distance entretous les vecteurs forme. C'est cette derni\`ere approchequi justifie la pr\'esence de cette section dans ces notes.\end{itemize}Parmi les techniques de positionnement multidimensionel, lesapproches m\'etriques sont couramment oppos\'ees aux approchesnon m\'etriques. Les premi\`eres produisent des repr\'esentationspr\'eservant au mieux l'information quantitative \cite{Sammon1969}contenue dans les donn\'ees, alors que les secondes privil\'egient l'information qualitative \cite{Kruskal1964}.    \subsection{Projection de Sammon}La projection de Sammon est une m\'ethode m\'etrique tr\`espopulaire. Cette m\'ethode cherche \`a ``projeter'' nonlin\'eairement les vecteurs formes $\x_1, \cdots, \x_N$ dans un espace deplus faible dimension ($\R^1$, $\R^2$, ou bien $\R^3$). Notons\begin{itemize}\item$\y_1, \cdots, \y_N$ les repr\'esentations des $\x_i$ dans cetespace de faible dimension,\item$\delta_{ij}$, la distance (ou bien dissimilarit\'e) entre les vecteursformes $\x_i$ et $\x_j$,\item$d_{ij}$, la distance (euclidienne ou autre) entre  $\y_i$ et $\y_j$.  \end{itemize}Le crit\`ere propos\'e par \citeasnoun{Sammon1969} pour juger de la qualit\'e de la repr\'esentation est le suivant : \begin{equation}S=\frac{1}{\sum_{i>j} \delta_{ij}} \sum_{i>j}\frac{(\delta_{ij}-d_{ij})^2}{\delta_{ij}},\end{equation}Remarquons que ce crit\`ere prend en compte les erreurs commises sur lespetites distances, contrairement \`a une ACP. Il est normalis\'e de mani\`ere\`a \^etre invariant pour des rotations, translations et changements d'\'echelles.La recherche de la configuration optimale des $\y_i$ peut s'effectuerpar une descente de gradient$$\y_k^{q+1}= \y_k^q -\alpha_q  \nabla_{y_k}^t S$$Par exemple, si $d_{ij}$ est la distance euclidienne,le gradient du crit\`ere par rapport au point $\y_k$ prend la forme suivante :$$\nabla_{y_k}^t S = \frac{1}{\sum_{i>j} \delta_{ij}} \sum_{j \neq k} \frac{(d_{kj}-\delta_{kj})}{\delta_{kj}} \cdot \frac{(\y_{k}-\y_{j})}{d_{kj}} $$La configuration initiale des $\y_i$ peut \^etre choisie au hasard mais leprocessus semble converger plus vite si l'on part d'une solution approch\'ee (celle obtenue par ACP par exemple.)\subsection{Projection de Kruskal}L'approche non m\'etrique favorise les repr\'esentations qui privil\'egient l'ordre relatif entre les dissimilarit\'es initiales plut\^ot que les valeurs.  Reprenons les notations utilis\'ees pourla projection de Sammon, en y ajoutant les $\hat{d}_{ij}$ qui sontdes nombres v\'erifiant la contrainte suivante : si les dissimilarit\'es initiales sont ordonn\'ees$$\delta_{i_1 j_1} \leq \cdots \leq \delta_{i_N j_N},$$alors on a$$\hat{d}_{i_1 j_1} \leq \cdots \leq \hat{d}_{i_N j_N}.$$\citeasnoun{Kruskal1964} propose de trouver une configuration des$\y_i$ telle que les distances $d_{ij}=dist(\y_i,\y_j)$ soientoptimales au sens du crit\`ere$$S=\sqrt{\frac{S^*}{T^*}}=\sqrt{\frac{\sum_{i>j}(d_{ij}-\hat{d}_{ij})^2}{\sum_{i>j} d_{ij}^2}}$$ L'optimisation de ce crit\`ere est plus d\'elicat que  celui deSammon. Remarquons d'ailleurs que bien que l'article deSammon soit post\'erieur de cinq ans, son crit\`ere etla m\'ethode d'optimisation propos\'ee constituent un cas particulier de l'approche de Kruskal.Kruskal propose une m\'ethode d'optimisation altern\'eepour optimiser les crit\`eres. Chaque it\'erationse partage en deux \'etapes :\begin{itemize}\item une r\'egression isotonique est utilis\'ee pour calculer les$\hat{d_{ij}}$ qui minimisent le crit\`ere (on consid\`ere les $d_{ij}$ fix\'es),et respectent la contrainte de montonicit\'e.\item le gradient du crit\`ere par rapport \`a chaque $\y_i$ est calcul\'e,et une nouvelle configuration $\y_1, \cdot, \y_N$ est obtenue en modifiantles points dans la direction du gradient (un pas d'une descente de gradient).%Si $d_{ij}$ est la distance euclidienne, on aura par exemple :%$$%\nabla_{y_k}S= S \sum_{i>j} \delta_{ij}} \sum_{j \neq k} \frac{(d_{kj}-\delta_{kj})}{\delta_{kj}} %\cdot \frac{(\y_{k}-\y_{j})}{d_{kj}} %$$\end{itemize}\section{S\'election de caract\'eristiques}Dans cette section sont pr\'esent\'ees quelques m\'ethodes qui permettentde s\'electionner $q$ variables parmi les  $d$ disponibles. Le butconsiste \`a garder les variables les plus discriminantes, c'est-\`a-dire,les variables qui pour un classifieur donn\'e vont produire le tauxd'erreur le plus petit possible. Ce probl\`eme peut \^etre partag\'een deux sous probl\`emes distincts :\begin{itemize}\itemd'une part, il faut disposer d'un crit\`ere pour juger laqualit\'e d'un groupe de variables,\itemd'autre part, il est n\'ecessaire d'utiliser des heuristiquespour optimiser ce crit\`ere. En pratique, il est en effet souventimpossible d'explorer tous les groupe des $q$ variables parmi $d$. \end{itemize}\subsection{Crit\`eres de choix}Le crit\`ere, qui mesure la performance d'un classifieur, est lerisque total $E^*$, qui est \'equivalent \`a la probabilit\'e d'erreur dans le cas o\`u le co\^ut $\{0,1\}$ est utilis\'e. Ce crit\`ere para\^{\i}t constituer un choix logique pour juger de la qualit\'ed'un groupe de variables, mais en pratique, de nombreuses autresmesures plus rapides \`a estimer ont \'et\'e propos\'ees :    \begin{itemize}\item$C_1=trace \left [  \bS_{Wq}^{-1} \bS_{Bq} \right ]=\sum_{i=1}^q \lambda_i$. Ce crit\`ere vient de l'analyse factorielle discriminante. Plus il est grand et plus le groupe de $q$ variables consid\'er\'e estdiscriminant au sens de l'AFD. C'est donc une mesure de la s\'eparabilit\'elin\'eaire para rapport au groupe de variables. \item$C_2=\frac{|\bS_{Wq}|}{|\bS_{Tq}|}=\prod_{i=1}^{q} \frac{1}{1+\lambda_i}$ (lambdade Wilks). Comme pr\'ec\'edemment ce crit\`ere peut s'interpr\'eterdans le cadre de l'AFD. Remarquons que minimiser $C_2$ revient \`amaximiser $C_1$.\item $C_3=[(\bmu_1 - \bmu_2)^t \bSigma^{-1} (\bmu_1 - \bmu_2)]^{\frac{1}{2}}$(distance de Mahalanobis). Dans le cadre limit\'e de la discriminationlin\'eaire entre deux classes, l'erreur commise ne d\'epend que de cettedistance (Chapitre 2, exemple \ref{ex:erreur}). Notonsque dans ce cas de figure, $C_1$, $C_2$ et $C_3$ sont \'equivalents.\item$C_4=\sqrt{p_1 p_2} \int \sqrt{f_1(\x) f_2(\x) }d\x$ (distance de Bhattacharya).Dans le  cas de deux classes, le risque de Bayes est born\'e sup\'erieurementpar cette distance :\begin{eqnarray*}E^* & = &   \E\left [ R(\hat{c}(X)|X) \right ]  \\    & = & \int \min \left ( \pi(1 |\x) ,\pi(2 |\x) \right ) f(\x) d\x \\    & = & \int \min \left ( p_1 f_1(\x) ,p_2 f_2(\x) \right ) d\x \\    & \leq &  \sqrt{p_1 p_2} \int \sqrt{f_1(\x) f_2(\x) }d\x\end{eqnarray*}car $min(a,b)\leq a^s b^{1-s}$. Remarquons encore que dans le cas de la discriminationlin\'eaire, ce crit\`ere est \'equivalent aux trois pr\'ec\'edents.\end{itemize} \subsection{Proc\'edures de s\'election}Les heuristiques de s\'election de variables comparent \`a chaque it\'eration deux groupes distincts par rapport \`a la valeur d'un crit\`ere, etretiennent le meilleur groupe au sens de ce crit\`ere.La s\'election ascendante commence par consid\'erer toutes les variabless\'eparement. La meilleure variable (au sens du crit\`ere choisi) est s\'electionn\'ee lors de la premi\`ere \'etape. La seconde \'etape cherchequelle variable ajout\'ee \`a la premi\`ere produit la meilleure valeurdu crit\`ere. Ce processus de construction est poursuivi jusqu'\`a obtenirles $q$ variables souhait\'ees. Notons que cette heuristique est optimale\`a chaque \'etape, mais ne garantit pas que le groupe de $q$ variablesfinal soit le meilleur. Ceci est vrai seulement si le crit\`ere peut sed\'ecomposer en somme ou produit dont chaque terme ou facteur estfonction d'une seule variable.La s\'election descendante commence par consid\'erer toutes les variables,et les \'elimine une par une. La variable \'elimin\'ee \`a chaque \'etapeest \'evidemment la moins bonne au sens du crit\`ere. % rajouter un passage sur branch and bound